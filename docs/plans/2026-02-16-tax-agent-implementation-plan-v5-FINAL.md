---
status: REVIEWED âœ…
date: 2026-02-16
reviewer: Antigravity (Opus 4) + Claude Code (Adversarial QA)
total_tests: 73
total_tasks: 8 (19 sub-tasks)
phase: 1 â€” MVP
review_log:
  - "v4 â†’ v5: 7 Intelligence Layers + Claude Code adversarial review (3ğŸ”´ + 5ğŸŸ¡ + 4ğŸŸ¢)"
  - "v5 final: SSE streaming, session persistence, auth middleware added to Phase 1 (Task 7 expanded 7â†’15 tests)"
  - "Coherence review: all task dependencies verified, Phase 2 roadmap updated"
  - "v5.1: MongoDB gap analysis â€” added database name, embedding_model tracking, session/api_keys collections, metadata completeness"
---

# Opus Plan v5: Georgian Tax AI Agent (Python)

> **Confidence: 94%** (â†’ 95%+ after Task 3.0 spike) | Size: XL | Approach: **Independent Python Service** reusing Scoop patterns
>
> **v5:** 7 Intelligence Layers + Claude Code adversarial review fixes applied (3ğŸ”´ + 5ğŸŸ¡ + 4ğŸŸ¢)

---

## Key Change: Node.js â†’ Python

The existing Scoop Python backend provides ~90% of the infrastructure needed. Instead of building from scratch in Node.js, we **copy proven patterns** into an independent service.

## Reuse Map (from `/backend/`)

| Scoop Component | What We Copy | Tax Agent Adaptation |
|---|---|---|
| [config.py](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/config.py) | BaseModel + os.getenv() pattern | Remove Scoop settings, add `embedding_dimensions`, `chunk_size`, `SIMILARITY_THRESHOLD=0.65` |
| [database.py](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/app/memory/database.py) | Singleton `DatabaseManager` (motor) | ğŸŸ¡ **COPY & ADAPT** â€” replace `_create_indexes()`, remove Scoop constants |
| [gemini_adapter.py](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/app/adapters/gemini_adapter.py) | `call_with_retry()`, existing `embed_content()` | ğŸŸ¡ Keep `embed_content()` (line 587), add only `embed_batch()` |
| [conversation_store.py](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/app/memory/conversation_store.py) | Session CRUD, history chain (~150 lines) | ğŸŸ¡ **EXTRACT** from 554-line file, not mongo_adapter.py |
| [georgian_normalizer.py](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/app/memory/georgian_normalizer.py) | `strip_georgian_suffix()` | Reuse for query normalization |
| [auth/dependencies.py](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/app/auth) | API key validation middleware | ğŸŸ¢ **COPY AS-IS** â€” domain-agnostic header check |
| [main.py StreamingResponse](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/main.py) | SSE event generator pattern | ğŸŸ¡ **COPY & ADAPT** â€” change event types: `products`â†’`sources`, `tip`â†’`disclaimer` |
| [Dockerfile](file:///Users/maqashable/Desktop/scoop-sagadasaxado/backend/Dockerfile) | Multi-stage build, Cloud Run pattern | Lighter requirements (no spacy, torch, etc.) |
| `requirements.txt` | Subset of 233 packages | ~16 packages: `fastapi`, `motor`, `google-genai`, `beautifulsoup4`, `structlog`, `pydantic`, etc. |

---

## Project Structure

```
scoop-sagadasaxado/tax-agent/        â† NEW independent directory
â”œâ”€â”€ main.py                          â† FastAPI entry (copied pattern from Scoop)
â”œâ”€â”€ config.py                        â† Pydantic Settings (adapted)
â”œâ”€â”€ requirements.txt                 â† Lean (~16 packages)
â”œâ”€â”€ Dockerfile                       â† Cloud Run ready (Python 3.11)
â”œâ”€â”€ .env.example                     â† All env vars documented
â”œâ”€â”€ .gitignore                       â† Python + venv + .env
â”œâ”€â”€ pyproject.toml                   â† ruff + black config
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py                  â† COPY of Scoop's DatabaseManager
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dependencies.py          â† COPY from Scoop (API key validation)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ tax_article.py           â† NEW: Pydantic model + Mongo ops
â”‚   â”‚   â””â”€â”€ definition.py            â† NEW: áƒ¢áƒ”áƒ áƒ›áƒ˜áƒœáƒ—áƒ áƒ’áƒáƒœáƒ›áƒáƒ áƒ¢áƒ”áƒ‘áƒ”áƒ‘áƒ˜
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ matsne_scraper.py        â† NEW: BeautifulSoup + aiohttp
â”‚   â”‚   â”œâ”€â”€ embedding_service.py     â† NEW: gemini-embedding-001 (768d, batch)
â”‚   â”‚   â”œâ”€â”€ vector_search.py         â† NEW: $vectorSearch + hybrid + cross-ref
â”‚   â”‚   â”œâ”€â”€ definition_resolver.py   â† NEW: áƒ¢áƒ”áƒ áƒ›áƒ˜áƒœáƒáƒšáƒáƒ’áƒ˜áƒ£áƒ áƒ˜ áƒ“áƒáƒ›áƒ˜áƒ¬áƒ”áƒ‘áƒ
â”‚   â”‚   â””â”€â”€ tax_agent.py             â† NEW: RAG pipeline + guardrails
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ georgian_normalizer.py   â† COPY from Scoop
â”‚   â”‚   â””â”€â”€ retry.py                 â† COPY call_with_retry pattern
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ api.py                   â† NEW: /ask, /ask/stream, /sessions, /articles, /health
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ tax_system_prompt.py         â† NEW: Georgian tax expert + lex specialis
â””â”€â”€ scripts/
    â”œâ”€â”€ seed_database.py             â† NEW: Full scrape + embed + insert
    â””â”€â”€ sync_matsne.py               â† NEW: Incremental update
```

### Project-Level Config (G5-G7)
```
Python: 3.11+
Linter: ruff (pyproject.toml)
Formatter: black (pyproject.toml)
CORS: CORSMiddleware(allow_origins=["*"]) â€” restrict in prod

.env.example:
  MONGODB_URI=mongodb+srv://...
  DATABASE_NAME=georgian_tax_db       â† NEW: explicit database name (same Atlas cluster, separate DB)
  GEMINI_API_KEY=...
  API_KEY_SECRET=...                  â† for hashing API keys (copy Scoop pattern)
  EMBEDDING_MODEL=text-embedding-004  â† tracked on every embedded document for migration safety
  SIMILARITY_THRESHOLD=0.65
  MATSNE_REQUEST_DELAY=2.0
  SEARCH_LIMIT=5
  RATE_LIMIT=30/minute
```

### MongoDB Database: `georgian_tax_db`

> [!IMPORTANT]
> All collections live in a **new, independent database** on the same Atlas cluster.
> Existing `scoop_db` and `scoop_ai` databases are **NOT modified**.

| Collection | Purpose | Vector Index | Phase |
|---|---|---|---|
| `tax_articles` | Parsed tax code articles (300+) | âœ… `tax_articles_vector_index` (768d cosine) | 1 |
| `definitions` | Legal term definitions (articles 1-8) | âœ… `definitions_vector_index` (768d cosine) | 1 |
| `metadata` | Version tracking, scrape status | â€” | 1 |
| `conversations` | Session history (adapted from Scoop) | â€” | 1 |
| `api_keys` | API key enrollment (copied from Scoop) | â€” | 1 |
| `sub_legislative` | Sub-legislative acts | TBD | 2 |

---

## Smart Retrieval Strategy (Core Intelligence)

> [!IMPORTANT]
> This is what separates a **valuable assistant** from a **dumb GPT wrapper**.

### Layer 1: Hierarchical Context ("áƒ›áƒ¨áƒáƒ‘áƒ”áƒšáƒ˜-áƒ¨áƒ•áƒ˜áƒšáƒ˜")

**Problem:** áƒ›áƒ£áƒ®áƒšáƒ¡ "áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜" áƒ”áƒ«áƒáƒ®áƒ˜áƒ¡ 3 áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ áƒ—áƒáƒ•áƒ˜. Naive search áƒáƒ£áƒ áƒ”áƒ•áƒ¡.

**Solution:** Embedding chunks include parent hierarchy:
```python
# BAD: "áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜ áƒáƒ áƒ˜áƒ¡ 18%"
# GOOD: "áƒ™áƒáƒ áƒ˜ IX. áƒ“áƒ¦áƒ’ â†’ áƒ—áƒáƒ•áƒ˜ I â†’ áƒ›áƒ£áƒ®áƒšáƒ˜ 169. áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜ áƒáƒ áƒ˜áƒ¡ 18%"
embedding_text = f"{kari} â†’ {tavi} â†’ áƒ›áƒ£áƒ®áƒšáƒ˜ {number}. {title}\n{body}"
```

The `TaxArticle` model stores: `kari`, `tavi`, `article_number`, `title`, `body` â€” and the embedding is built from **all of them concatenated**.

### Layer 2: Cross-References ("áƒ¯áƒ•áƒáƒ áƒ”áƒ“áƒ˜áƒœáƒ˜ áƒ‘áƒ›áƒ£áƒšáƒ”áƒ‘áƒ˜")

**Problem:** áƒ›áƒ£áƒ®áƒšáƒ˜ 98: "áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜áƒ áƒ›áƒ”-100 áƒ›áƒ£áƒ®áƒšáƒ˜áƒ— áƒ’áƒáƒ—áƒ•áƒáƒšáƒ˜áƒ¡áƒ¬áƒ˜áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒ”áƒ‘áƒ˜."

**Solution:** Scraper extracts `related_articles` via regex:
```python
import re
# ğŸ”´ FIX (C1): non-capturing group, not character class + all Georgian cases
REF_PATTERN = re.compile(r'áƒ›áƒ£áƒ®áƒš(?:áƒ˜|áƒ˜áƒ¡|áƒ˜áƒ—|áƒ¨áƒ˜|áƒ–áƒ”|áƒ˜áƒ¡áƒ)\s*(\d+)')
ORD_REF_PATTERN = re.compile(r'áƒ›áƒ”-?(\d+)\s*áƒ›áƒ£áƒ®áƒš')  # ordinal: áƒ›áƒ”-100 áƒ›áƒ£áƒ®áƒšáƒ˜

def extract_cross_refs(body: str) -> List[int]:
    refs = set(REF_PATTERN.findall(body) + ORD_REF_PATTERN.findall(body))
    return [int(r) for r in refs]
```

Retrieval fetches **primary + related**:
```python
async def search_with_refs(query: str) -> List[TaxArticle]:
    primary = await vector_search(query, limit=5)
    ref_ids = set()
    for art in primary:
        ref_ids.update(art.related_articles)
    related = await find_by_numbers(list(ref_ids))
    return merge_and_rank(primary, related)
```

### Layer 3: Conditional Logic (Multi-Fact Retrieval)

**Problem:** "áƒ›áƒªáƒ˜áƒ áƒ” áƒ‘áƒ˜áƒ–áƒœáƒ”áƒ¡áƒ˜ áƒ•áƒáƒ  áƒ“áƒ áƒ‘áƒ˜áƒœáƒáƒ¡ áƒ•áƒáƒ¥áƒ˜áƒ áƒáƒ•áƒ”áƒ‘. 1%-áƒ¡ áƒ•áƒ˜áƒ®áƒ“áƒ˜?"

**Solution:** RAG prompt instructs model to check **both** conditions:
```
SYSTEM: When the user describes a multi-condition scenario,
you MUST check each condition against separate articles.
Never answer based on just one matching article.
```

Hybrid search catches this â€” query decomposes into 2 semantic searches:
1. "áƒ›áƒªáƒ˜áƒ áƒ” áƒ‘áƒ˜áƒ–áƒœáƒ”áƒ¡áƒ˜áƒ¡ áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜" â†’ finds rate article
2. "áƒ›áƒªáƒ˜áƒ áƒ” áƒ‘áƒ˜áƒ–áƒœáƒ”áƒ¡áƒ˜áƒ¡ áƒáƒ™áƒ áƒ«áƒáƒšáƒ£áƒšáƒ˜ áƒ¡áƒáƒ¥áƒ›áƒ˜áƒáƒœáƒáƒ‘áƒ" â†’ finds exclusions

### Red Zone Guardrails (Hardcoded Refusal)

| Category | Example | Response |
|---|---|---|
| Tax optimization | "áƒ áƒáƒ’áƒáƒ  áƒ“áƒáƒ•áƒ›áƒáƒšáƒ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ”áƒ‘áƒ˜?" | "áƒ›áƒ” áƒ•áƒáƒ  áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ¡ áƒ˜áƒœáƒ¢áƒ”áƒ áƒáƒ áƒ”áƒ¢áƒáƒ¢áƒáƒ áƒ˜. áƒ—áƒáƒ•áƒ˜áƒ¡ áƒáƒ áƒ˜áƒ“áƒ”áƒ‘áƒ áƒ™áƒáƒœáƒáƒœáƒ“áƒáƒ áƒ¦áƒ•áƒ”áƒ•áƒáƒ." |
| Legal disputes | "áƒáƒáƒ áƒ¢áƒœáƒ˜áƒáƒ áƒ˜ áƒ¤áƒ£áƒšáƒ¡ áƒ›áƒáƒáƒ áƒáƒ•áƒ¡" | "áƒ”áƒ¡ áƒ¡áƒªáƒ“áƒ”áƒ‘áƒ áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ¡ áƒ¤áƒáƒ áƒ’áƒšáƒ”áƒ‘áƒ¡. áƒ›áƒ˜áƒ›áƒáƒ áƒ—áƒ”áƒ— áƒ˜áƒ£áƒ áƒ˜áƒ¡áƒ¢áƒ¡." |
| Calculations | "5000 áƒšáƒáƒ áƒ˜ áƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒáƒšáƒ˜, áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ˜?" | "áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜ 20%-áƒ˜áƒ (áƒ›áƒ£áƒ®áƒšáƒ˜ 81). áƒ™áƒáƒšáƒ™áƒ£áƒšáƒáƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ— rs.ge." |
| Medical/Criminal | Anything non-tax | "áƒ•áƒáƒáƒ¡áƒ£áƒ®áƒáƒ‘ áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ—áƒáƒœ áƒ“áƒáƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ”áƒ‘áƒ£áƒš áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ”áƒ‘áƒ¡." |

Implemented as **pre-retrieval classifier** in `tax_agent.py`:
```python
GUARDRAIL_PATTERNS = [
    (r'áƒ“áƒáƒ›áƒáƒš|áƒ—áƒáƒ•áƒ˜áƒ¡ áƒáƒ áƒ˜áƒ“áƒ”áƒ‘áƒ|áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜áƒ¡ áƒáƒ•áƒš', 'tax_evasion'),
    (r'áƒ™áƒáƒšáƒ™áƒ£áƒšáƒáƒªáƒ˜|áƒ’áƒáƒ›áƒáƒ—áƒ•áƒáƒš|áƒ áƒáƒ›áƒ“áƒ”áƒœ.*áƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“', 'calculation'),
    (r'áƒáƒáƒ áƒ¢áƒœáƒ˜áƒáƒ .*áƒ›áƒáƒáƒ áƒáƒ•áƒ¡|áƒ¡áƒáƒ áƒ©áƒ”áƒš|áƒ¡áƒ˜áƒ¡áƒ®áƒšáƒ˜áƒ¡', 'legal_dispute'),
]
```

### Layer 5: Terminological Grounding ("áƒ“áƒáƒ›áƒ˜áƒ¬áƒ”áƒ‘áƒ") â€” Phase 1 âœ…

**Problem:** "áƒáƒ˜áƒ áƒ˜" â‰  áƒáƒ“áƒáƒ›áƒ˜áƒáƒœáƒ˜. "áƒ“áƒáƒ¡áƒáƒ‘áƒ”áƒ’áƒ áƒ˜ áƒáƒ˜áƒ áƒ˜" â‰  "áƒ¤áƒ˜áƒ–áƒ˜áƒ™áƒ£áƒ áƒ˜ áƒáƒ˜áƒ áƒ˜". áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ¡ **áƒ—áƒáƒ•áƒ˜áƒ¡** áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ™áƒáƒœáƒ˜ áƒáƒ¥áƒ•áƒ¡.

**Solution:** Separate `definitions` collection from articles 1â€“8:
```python
# definitions collection
{
    "term": "áƒ“áƒáƒ¡áƒáƒ‘áƒ”áƒ’áƒ áƒ˜ áƒáƒ˜áƒ áƒ˜",
    "definition": "áƒáƒ˜áƒ áƒ˜, áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª áƒ”áƒ™áƒáƒœáƒáƒ›áƒ˜áƒ™áƒ£áƒ  áƒ¡áƒáƒ¥áƒ›áƒ˜áƒáƒœáƒáƒ‘áƒáƒ¡ áƒ”áƒ¬áƒ”áƒ•áƒ...",
    "article_ref": 157,
    "embedding": [...]  # 768d
}
```

**Query pre-processing (ğŸŸ¡ W4 fix: vector search instead of $text):**
```python
async def resolve_terms(query: str) -> str:
    """Inject legal definitions before RAG search.
    Uses $vectorSearch instead of $text â€” MongoDB has no Georgian stemming."""
    query_embedding = await embed_content(query)
    matched = await definitions_collection.aggregate([
        {"$vectorSearch": {
            "queryVector": query_embedding,
            "path": "embedding",
            "numCandidates": 20,
            "limit": 3,
            "index": "definitions_vector_index"
        }}
    ]).to_list(3)
    if matched:
        context = "\n".join(f"áƒ¢áƒ”áƒ áƒ›áƒ˜áƒœáƒ˜: {d['term']} = {d['definition']}" for d in matched)
        return f"{context}\n\náƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: {query}"
    return query
```

### Layer 6: Lex Specialis â€” "áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ áƒ˜ áƒ¯áƒáƒ‘áƒœáƒ˜áƒ¡ áƒ–áƒáƒ’áƒáƒ“áƒ¡" â€” Phase 1 âœ…

**Problem:** "áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ áƒ¡áƒáƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒšáƒ?" â†’ "20%" áƒáƒ áƒáƒ¡áƒ áƒ£áƒšáƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ. áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ”áƒ‘áƒ˜áƒª áƒ£áƒœáƒ“áƒ.

**Solution â€” Dual approach:**

**A. Metadata tagging** during scraping:
```python
# If article body contains exception markers, flag it
EXCEPTION_MARKERS = ["áƒ’áƒáƒ áƒ“áƒ", "áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜", "áƒáƒ  áƒ•áƒ áƒªáƒ”áƒšáƒ“áƒ”áƒ‘áƒ", "áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ áƒ˜"]
article.is_exception = any(m in article.body for m in EXCEPTION_MARKERS)
```

**B. Re-ranking** in search:
```python
def rerank_with_exceptions(results: List[TaxArticle]) -> List[TaxArticle]:
    """Attach exception articles RIGHT AFTER their general rules."""
    general = [r for r in results if not r.is_exception]
    exceptions = [r for r in results if r.is_exception]
    # NOT interleave â€” attach exceptions after related generals
    ranked = []
    for g in general:
        ranked.append(g)
        for e in exceptions:
            if g.article_number in e.related_articles:
                ranked.append(e)
    return ranked
```

**C. System prompt:**
```
áƒ”áƒ¡ áƒáƒ áƒ˜áƒ¡ áƒ¡áƒáƒ›áƒáƒ áƒ—áƒšáƒ˜áƒ¡ áƒáƒ¥áƒ áƒáƒ¡ áƒ¬áƒ”áƒ¡áƒ˜: áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ áƒ˜ áƒœáƒáƒ áƒ›áƒ áƒ¯áƒáƒ‘áƒœáƒ˜áƒ¡ áƒ–áƒáƒ’áƒáƒ“áƒ¡.
áƒ áƒáƒªáƒ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ¡ áƒáƒáƒ¡áƒ£áƒ®áƒáƒ‘, ALWAYS mention:
1. áƒ–áƒáƒ’áƒáƒ“áƒ˜ áƒ¬áƒ”áƒ¡áƒ˜ (General rule)
2. áƒáƒ áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ”áƒ‘áƒ˜ (Exceptions)
3. áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ áƒ˜ áƒ áƒ”áƒŸáƒ˜áƒ›áƒ”áƒ‘áƒ˜ (Special regimes)
```

### Layer 7: Temporal Awareness ("áƒ“áƒ áƒáƒ˜áƒ—áƒ˜ áƒ›áƒáƒœáƒ¥áƒáƒœáƒ") â€” Phase 1 LITE âš¡

**Problem:** "2022 áƒ¬áƒ”áƒšáƒ¡ áƒ’áƒáƒ•áƒ§áƒ˜áƒ“áƒ” áƒ‘áƒ˜áƒœáƒ" â€” áƒ™áƒáƒœáƒáƒœáƒ˜ áƒ›áƒáƒ¡ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’ áƒ¨áƒ”áƒ˜áƒªáƒ•áƒáƒšáƒ.

**Phase 1 (LITE):** áƒ©áƒ•áƒ”áƒœ áƒ•áƒáƒáƒ áƒ¡áƒáƒ•áƒ— **áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ›áƒáƒ¥áƒ›áƒ”áƒ“** áƒ™áƒáƒœáƒ¡áƒáƒšáƒ˜áƒ“áƒ˜áƒ áƒ”áƒ‘áƒ£áƒš áƒ•áƒ”áƒ áƒ¡áƒ˜áƒáƒ¡. áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ˜áƒ¡áƒ¢áƒáƒ áƒ˜áƒ£áƒšáƒ˜ áƒ«áƒ˜áƒ”áƒ‘áƒ áƒáƒ  áƒáƒ áƒ˜áƒ¡.

**áƒ áƒáƒ¡ áƒ•áƒáƒ™áƒ”áƒ—áƒ”áƒ‘áƒ— Phase 1-áƒ¨áƒ˜:**
```python
# Scraper extracts amendment dates from <a class="DocumentLink">
article.last_amended_date = extract_latest_amendment(siblings)  # "2024-03-15"

# Agent detects past-tense queries:
PAST_DATE_PATTERN = re.compile(r'(20\d{2})\s*áƒ¬')
if match := PAST_DATE_PATTERN.search(query):
    year = int(match.group(1))
    disclaimer = f"âš ï¸ áƒ”áƒ¡ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ”áƒ¤áƒ£áƒ«áƒœáƒ”áƒ‘áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ¡ áƒ›áƒáƒ¥áƒ›áƒ”áƒ“ áƒ áƒ”áƒ“áƒáƒ¥áƒªáƒ˜áƒáƒ¡. "
                 f"{year} áƒ¬áƒ”áƒšáƒ¡ áƒ›áƒáƒ¥áƒ›áƒ”áƒ“áƒ˜ áƒ áƒ”áƒ“áƒáƒ¥áƒªáƒ˜áƒ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ’áƒáƒœáƒ¡áƒ®áƒ•áƒáƒ•áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ˜áƒ§áƒáƒ¡."
```

**Phase 2 (FULL):** Matsne publication API-áƒ˜áƒ— áƒ˜áƒ¡áƒ¢áƒáƒ áƒ˜áƒ£áƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ”áƒ‘áƒ˜áƒ¡ `valid_from`/`valid_to`.

### Sub-Legislative Acts Reference â€” Phase 1 LITE âš¡

**Problem:** áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜ = áƒ©áƒáƒœáƒ©áƒ®áƒ˜. áƒ¤áƒ˜áƒœáƒáƒœáƒ¡áƒ—áƒ áƒ›áƒ˜áƒœáƒ˜áƒ¡áƒ¢áƒ áƒ˜áƒ¡ áƒ‘áƒ áƒ«áƒáƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜ = áƒ®áƒáƒ áƒªáƒ˜.

**Phase 1 (LITE):** Top-10 áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ®áƒ¨áƒ˜áƒ áƒáƒ“ áƒ›áƒáƒ®áƒ¡áƒ”áƒœáƒ˜áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¥áƒ•áƒ”áƒ™áƒáƒœáƒáƒœáƒ£áƒ áƒ˜ áƒáƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ **áƒªáƒœáƒáƒ‘áƒáƒ áƒ˜**:
```python
SUB_LEGISLATIVE_REFS = {
    "415": {"title": "áƒ›áƒ—áƒáƒ•áƒ áƒáƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ“áƒ’áƒ”áƒœáƒ˜áƒšáƒ”áƒ‘áƒ â„–415", "topic": "áƒ›áƒªáƒ˜áƒ áƒ” áƒ‘áƒ˜áƒ–áƒœáƒ”áƒ¡áƒ˜áƒ¡ áƒáƒ™áƒ áƒ«áƒáƒšáƒ£áƒšáƒ˜ áƒ¡áƒáƒ¥áƒ›áƒ˜áƒáƒœáƒáƒ‘áƒ”áƒ‘áƒ˜"},
    "996": {"title": "áƒ‘áƒ áƒ«áƒáƒœáƒ”áƒ‘áƒ â„–996", "topic": "áƒ¡áƒáƒ¥áƒáƒœáƒšáƒ˜áƒ¡ áƒ©áƒáƒ›áƒáƒ¬áƒ”áƒ áƒ˜áƒ¡ áƒáƒ áƒáƒªáƒ”áƒ“áƒ£áƒ áƒ"},
    # ... 8 more
}
# When citing article that references these â†’ add note
```

**Phase 2 (FULL):** áƒªáƒáƒšáƒ™áƒ” Matsne scraper áƒ¥áƒ•áƒ”áƒ™áƒáƒœáƒáƒœáƒ£áƒ áƒ˜ áƒáƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ â†’ `sub_legislative` collection.

---

## Risk Matrix

| Scenario | Prob. | Impact | Mitigation |
|---|---|---|---|
| Matsne HTML changes | HIGH | CRITICAL | Validation layer, canary queries, backup |
| Gemini API rate limit | HIGH (free) | HIGH | Embedding cache, backoff, RPM monitor |
| Wrong article cited | MEDIUM | CRITICAL | Similarity â‰¥0.75, hybrid search, disclaimer |
| Cross-ref missing | MEDIUM | HIGH | Regex extraction + manual review of top 50 articles |
| Georgian tokenization | MEDIUM | MEDIUM | Conservative batching (80%), `countTokens` |
| Guardrail bypass | LOW | HIGH | Regex + LLM double-check, logging all refusals |

## â›” Architecture Exclusion List â€” DO NOT CARRY OVER

> [!CAUTION]
> These Scoop-specific modules must **NOT** be copied, imported, or referenced in the Tax Agent.
> The Tax Agent is an **independent Python service** â€” it shares infrastructure patterns but NOT domain logic.

| Module | Lines | Why Excluded |
|--------|-------|-------------|
| `catalog/loader.py` | 16,968B | Product catalog â€” zero tax relevance |
| `reasoning/query_analyzer.py` | 21,099B | Shopping NLU â€” tax uses embedding search |
| `reasoning/constraint_search.py` | 10,142B | Product filters â€” no constraints in tax |
| `reasoning/context_injector.py` | 17,695B | Shopping context â€” tax builds own RAG context |
| `tools/user_tools.py` | 27,602B | Function calling â€” tax is pure RAG, no tools |
| `profile/profile_extractor.py` | 23,565B | Shopping preferences â€” no user profiles |
| `profile/profile_processor.py` | 6,759B | Belief decay â€” not needed |
| `core/types.py` | 400 lines | Scoop-specific: `EngineConfig`, `FunctionCall`, `LoopState` |
| `core/model_router.py` | 270 lines | Multi-model routing â€” tax uses single model |
| `cache/context_cache.py` | 15,279B | Gemini context caching â€” Phase 2 optimization |
| `adapters/mongo_adapter.py` | 641 lines | Delegation layer â€” extract from `conversation_store.py` instead |
| `memory/user_store.py` | 1,325 lines | User profiles â€” entirely Scoop-specific |
| `memory/mongo_store.py` | 53 lines | Facade re-exporting both stores â€” unnecessary |

**If you find yourself needing anything from these files, STOP and re-evaluate.** The tax agent has its own domain-specific implementations for search, context, and types.

---

## Implementation Tasks (Bite-Sized TDD)

### Task 1: Project Scaffold
Copy patterns, install deps, verify health endpoint.
```
â€¢ Copy config.py (keep BaseModel + os.getenv pattern â€” ğŸŸ¡ W1)
â€¢ Copy & ADAPT database.py (ğŸ”´ C2: replace _create_indexes(), remove Scoop constants)
â€¢ Copy app/auth/ directory from Scoop (API key validation middleware â€” domain-agnostic)
â€¢ Copy Dockerfile pattern (Python 3.11-slim)
â€¢ Create .gitignore, .env.example (incl. API_KEY_SECRET), pyproject.toml (ruff + black)
â€¢ EXTRACT ~150 lines from conversation_store.py (ğŸŸ¡ W3: not mongo_adapter.py)
â€¢ pip install fastapi motor google-genai beautifulsoup4 structlog pydantic slowapi
â€¢ Create main.py with /health endpoint + CORSMiddleware + auth middleware (ğŸŸ¢ S4)
â€¢ Verify: server starts, MongoDB connects, /health returns 200

DONE WHEN: `uvicorn main:app` starts, /health returns 200 with {"status":"ok"}, auth middleware active

Tests (5):
  1. Server starts on port 8000, /health returns {"status": "ok"} (200)
  2. /health with DB disconnected â†’ {"status": "degraded", "db_connected": false} (200, not 500)
  3. Missing MONGODB_URI env var â†’ server fails fast with clear error message
  4. CORS preflight OPTIONS request â†’ 200 with correct Access-Control headers
  5. Request without X-API-Key header to protected endpoint â†’ 401 Unauthorized
```

### Task 2: TaxArticle Model + Definitions + Atlas Index
```
â€¢ TaxArticle Pydantic model:
  - article_number: int (ge=1, le=500)
  - kari: str (áƒ™áƒáƒ áƒ˜ â€” Part)
  - tavi: str (áƒ—áƒáƒ•áƒ˜ â€” Chapter)  
  - title: str
  - body: str (min_length=10)
  - related_articles: List[int]  â† cross-references
  - is_exception: bool           â† lex specialis flag
  - last_amended_date: Optional[date]  â† temporal awareness
  - embedding: List[float]       â† 768d
  - embedding_model: str = "text-embedding-004"  â† ğŸ†• tracks which model generated this vector
  - status: enum (active | repealed | amended)
  - embedding_text: str          â† "áƒ™áƒáƒ áƒ˜ â†’ áƒ—áƒáƒ•áƒ˜ â†’ áƒ›áƒ£áƒ®áƒšáƒ˜ N. Title\nBody"
â€¢ Definition model:
  - term: str
  - definition: str
  - article_ref: int
  - embedding: List[float]
  - embedding_model: str = "text-embedding-004"  â† ğŸ†• same tracking for migration safety
â€¢ MongoDB CRUD: insert, upsert (keyed on article_number), find_by_number, find_by_numbers(list)
â€¢ Database: `georgian_tax_db` (same Atlas cluster, independent from scoop_db/scoop_ai)
â€¢ Collections: tax_articles + definitions + metadata + conversations + api_keys
â€¢ metadata stores: {"key": "tax_code_version", "publication": N, "last_checked": ISODate, "scrape_status": "completed", "total_articles": int, "embedding_model": "text-embedding-004"}
â€¢ Atlas Vector Index (create via Atlas UI/API, NOT driver):
  - tax_articles: 768d cosine, filter on status, index name "tax_articles_vector_index"
  - definitions: 768d cosine, index name "definitions_vector_index" (ğŸŸ¡ W4)
â€¢ Embedding dimension assertion: assert len(emb) == 768 (ğŸŸ¢ S1)

DONE WHEN: Models pass validation, CRUD tests pass, Atlas indexes created

Tests (6):
  1. TaxArticle(article_number=0) â†’ ValidationError (ge=1)
  2. TaxArticle(article_number=501) â†’ ValidationError (le=500)
  3. TaxArticle(body="") â†’ ValidationError (min_length=10)
  4. upsert(article_number=169, body="new") twice â†’ only 1 document in DB
  5. find_by_numbers([81, 82, 999]) â†’ returns 2 results (ignores non-existent)
  6. Definition model with empty term â†’ ValidationError
```

### Task 3.0: Matsne Fetch Validation Spike ğŸ”´ DO FIRST (~2 hours)
```
â€¢ Fetch raw HTML with aiohttp â€” check if #maindoc has content or is empty (JS-rendered)
â€¢ If JS-rendered â†’ add playwright to requirements, use async headless Chrome
â€¢ If server-rendered â†’ document exact DOM structure between oldStyleDocumentPart anchors
â€¢ Document body text extraction: what elements contain law text between headers
â€¢ This spike MUST complete before any Task 3 work begins
â€¢ Result determines: aiohttp+BS4 sufficient OR playwright needed

EXIT CRITERIA: Written doc (spike_result.md) confirming:
  1. Server-rendered OR JS-rendered (with proof: raw HTML snippet)
  2. Exact CSS selectors for: article headers, body text, amendment links
  3. Tool decision: aiohttp+BS4 OR playwright
```

### Task 3: Matsne Scraper âš ï¸ CRITICAL PATH

> Split into 6 atomic sub-tasks. Each can be tested independently.

#### Task 3a: Version Detection + HTML Fetch
```
â€¢ detect_latest_version(): follow redirect, extract publication from final URL
  1. GET https://matsne.gov.ge/ka/document/view/1043717 (no ?publication=)
  2. Matsne REDIRECTS to ?publication=N (latest version)
  3. Parse N from final URL â†’ use as current_publication
  4. Store N in metadata collection
â€¢ fetch_document_html(publication: int) â†’ str: aiohttp with rate limiting
â€¢ ğŸŸ¢ MATSNE_REQUEST_DELAY = 2.0s between requests (S2)

DONE WHEN: detect_latest_version() returns int, fetch returns >100KB HTML

Tests (4):
  1. detect_latest_version() â†’ returns int > 200 (current publication)
  2. fetch_document_html(239) â†’ returns string with len > 100_000
  3. fetch_document_html with network timeout â†’ raises RetryableError (not crash)
  4. Rate limiter enforces â‰¥2s gap between consecutive fetches
```

#### Task 3b: Header Parser (State Machine)
```
â€¢ Selector: soup.select("a.oldStyleDocumentPart") â€” flat list
â€¢ State machine: track current_kari, current_tavi as we iterate
â€¢ Parse: áƒ›áƒ£áƒ®áƒšáƒ˜ X â†’ article header, áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ â†’ repealed
â€¢ Output: List[{article_number, kari, tavi, title, status, anchor_element}]

DONE WHEN: Parses 300+ headers with correct hierarchy

Tests (5):
  1. Total parsed headers â‰¥ 300
  2. Each article has non-empty kari and tavi (hierarchy complete)
  3. No duplicate article_numbers in output
  4. "áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ" articles have status="repealed"
  5. Article 1 belongs to áƒ™áƒáƒ áƒ˜ I, áƒ—áƒáƒ•áƒ˜ I (first hierarchy check)
```

#### Task 3c: Body Text Extraction
```
â€¢ ğŸ”´ C3: after each áƒ›áƒ£áƒ®áƒšáƒ˜ header anchor,
  collect ALL sibling nodes until next oldStyleDocumentPart
  â†’ normalize whitespace, strip HTML, preserve paragraphs
  â†’ handle inline <a class="DocumentLink"> (amendment refs)
â€¢ Validation: reject empty bodies, assert body.strip() != ""

DONE WHEN: Every non-repealed article has body text >10 chars

Tests (4):
  1. Article 169 (short law text) â†’ body contains "18%" or known content
  2. No non-repealed article has empty body (assert all len > 10)
  3. DocumentLink <a> tags stripped from body text (no raw HTML)
  4. Body preserves paragraph breaks (\n between paragraphs, not collapsed)
```

#### Task 3d: Cross-Reference + Exception Extraction
```
â€¢ Extract cross-refs: regex r'áƒ›áƒ£áƒ®áƒš(?:áƒ˜|áƒ˜áƒ¡|áƒ˜áƒ—|áƒ¨áƒ˜|áƒ–áƒ”|áƒ˜áƒ¡áƒ)\s*(\d+)' (ğŸ”´ C1 fixed)
  + ordinal pattern r'áƒ›áƒ”-?(\d+)\s*áƒ›áƒ£áƒ®áƒš'
â€¢ Flag exceptions: detect "áƒ’áƒáƒ áƒ“áƒ", "áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜" â†’ is_exception=True
â€¢ Extract amendment dates from <a class="DocumentLink"> â†’ last_amended_date

DONE WHEN: Cross-ref count >500 total, exceptions detected in 20+ articles

Tests (5):
  1. Article 82 body contains "áƒ›áƒ£áƒ®áƒšáƒ˜ 81" â†’ related_articles includes 81
  2. Ordinal pattern: "áƒ›áƒ”-7 áƒ›áƒ£áƒ®áƒšáƒ˜" â†’ extracts article_number 7
  3. "áƒ’áƒáƒ áƒ“áƒ" in body â†’ is_exception = True
  4. Article with no cross-refs â†’ related_articles = [] (not None)
  5. Regex doesn't match non-article numbers (e.g., "2023 áƒ¬áƒ”áƒšáƒ˜" â‰  article 2023)
```

#### Task 3e: Definition Extraction
```
â€¢ Extract definitions from áƒ—áƒáƒ•áƒ˜ I (articles 1-8) â†’ definitions collection
â€¢ Parse term-definition pairs from article body structure
â€¢ Store with article_ref for traceability

DONE WHEN: 20+ definitions extracted from articles 1-8

Tests (3):
  1. Term "áƒ“áƒáƒ¡áƒáƒ‘áƒ”áƒ’áƒ áƒ˜ áƒáƒ˜áƒ áƒ˜" exists in definitions collection
  2. Each definition has article_ref pointing to valid article (1-8)
  3. No duplicate terms in definitions collection
```

#### Task 3f: Assembly + Validation
```
â€¢ Build embedding_text: "áƒ™áƒáƒ áƒ˜ â†’ áƒ—áƒáƒ•áƒ˜ â†’ áƒ›áƒ£áƒ®áƒšáƒ˜ N. Title\nBody"
â€¢ Assemble TaxArticle objects from 3b+3c+3d outputs
â€¢ ğŸ“Š Log article length distribution:
  - Buckets: <1K chars, 1K-5K, 5K-8K, >8K chars
â€¢ Partial failure handling: if 3c fails for article N, log warning + skip
  (don't fail entire pipeline for one broken article)
â€¢ Final validation: 100+ articles with hierarchy + cross-refs + exception flags + non-empty bodies

DONE WHEN: 300+ TaxArticle objects pass Pydantic validation, <5% skipped

Tests (3):
  1. Assembly output count â‰¥ 300 (enough articles)
  2. Skipped articles < 5% of total (partial failure tolerance)
  3. Every assembled article has non-empty embedding_text starting with "áƒ™áƒáƒ áƒ˜"
```

### Task 4: Embedding Pipeline
```
â€¢ Keep existing embed_content() from gemini_adapter.py (ğŸŸ¡ W2 â€” already at line 587)
â€¢ Add embedBatch(): up to 250 texts, â‰¤20K tokens per request
â€¢ buildEmbeddingText(): hierarchy prefix + article text concatenation
  Format: "áƒ™áƒáƒ áƒ˜ IX. áƒ“áƒ¦áƒ’ â†’ áƒ—áƒáƒ•áƒ˜ I â†’ áƒ›áƒ£áƒ®áƒšáƒ˜ 169. áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜\n18%-áƒ˜áƒ..."
â€¢ ğŸ”´ NEW â€” Token truncation safety:
  MAX_EMBEDDING_CHARS = 8000 (~2000 tokens for Georgian)
  if len(embedding_text) > MAX_EMBEDDING_CHARS:
      log warning + truncate (text-embedding-004 limit: 2048 tokens)
  Phase 2: split long articles into sub-article chunks instead of truncating
â€¢ ğŸŸ¢ Dimension assertion: assert len(embedding) == 768 (S1)
â€¢ Test: correct dimensions, batch limits, hierarchy preserved in embedding
â€¢ Test: article > 8000 chars â†’ truncated with warning, not error
```

### Task 5: Vector Search + Cross-Reference + Lex Specialis
```
â€¢ searchBySemantic(): $vectorSearch pipeline with EXPLICIT score projection:
  pipeline = [
    {"$vectorSearch": {
      "index": "tax_articles_vector_index",
      "path": "embedding",
      "queryVector": query_vector,
      "numCandidates": 100,           # configurable
      "limit": 5,                     # configurable via SEARCH_LIMIT
      "filter": {"status": "active"}  # ğŸ”´ PRE-filter (not post-filter $match)
    }},
    {"$project": {                    # ğŸ”´ CRITICAL: must project score
      "article_number": 1, "kari": 1, "tavi": 1, "title": 1, "body": 1,
      "related_articles": 1, "is_exception": 1,
      "score": {"$meta": "vectorSearchScore"}
    }}
  ]

â€¢ detectArticleNumber(): query-level regex (SEPARATE from cross-ref regex):
  patterns: "áƒ›áƒ£áƒ®áƒšáƒ˜ 98", "áƒ›áƒ”-98 áƒ›áƒ£áƒ®áƒšáƒ˜", bare "98" (validated 1-400 range)

â€¢ hybridSearch():
  IF detectArticleNumber(query) â†’ find_by_number() (skip vector search)
  ELSE â†’ searchBySemantic()

â€¢ Threshold filtering (post-pipeline in Python):
  filtered = [r for r in results if r["score"] >= SIMILARITY_THRESHOLD]
  if not filtered â†’ return "áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ” áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ áƒ˜ áƒ›áƒ£áƒ®áƒšáƒ˜" message

â€¢ enrichWithCrossRefs():
  - Collect ref_ids from primary results
  - ğŸ”´ EXCLUDE primary article IDs (already have them): ref_ids -= primary_ids
  - ğŸ”´ CAP at 10 cross-refs maximum: ref_ids = list(ref_ids)[:10]
  - Filter by status=active

â€¢ rerank_with_exceptions():
  ğŸ”´ NOT interleave â€” use "general + attached exceptions" pattern:
  For each general rule â†’ attach its related exceptions RIGHT AFTER it
  Preserve vectorSearchScore ordering within each group

â€¢ merge_and_rank(): deduplicate by article_number
â€¢ Cosine similarity â‰¥0.65 (ğŸŸ¡ W5: 0.75 too aggressive for Georgian morphology)
â€¢ Make threshold configurable via env var SIMILARITY_THRESHOLD
â€¢ Log similarity scores in structured format: {query, article_num, score}

Tests (8):
  1. "áƒ›áƒ£áƒ®áƒšáƒ˜ 98" â†’ regex path â†’ direct lookup â†’ article 98 + cross-refs (100)
  2. "áƒ¡áƒáƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒšáƒ áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜" â†’ vector path â†’ 20% general + 1% exception
  3. "áƒ‘áƒ˜áƒœáƒ áƒ áƒáƒ› áƒ’áƒáƒ•áƒ§áƒ˜áƒ“áƒ?" â†’ semantic â†’ article 168
  4. Irrelevant query â†’ all scores < 0.65 â†’ "áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ”" message
  5. Shared cross-refs â†’ no duplicate articles in result set
  6. Article referencing 20+ refs â†’ capped at 10 cross-refs
  7. Repealed article â†’ NOT returned even if semantically similar
  8. Verify structured log output includes {query, article_num, score}
```

### Task 6: Tax RAG Agent + Guardrails + Intelligence Layers

> Split into 5 atomic sub-tasks. Build bottom-up.

#### Task 6a: Pre-Retrieval Pipeline (3 classifiers)
```
â€¢ Red Zone classifier (regex): detect "áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜", "áƒ’áƒáƒ›áƒáƒ—áƒ•áƒáƒšáƒ”", "áƒ áƒ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ˜"
  â†’ sets disclaimer_needed = True
â€¢ Term resolver: lookup query terms in definitions collection
  â†’ enrich context with resolved definitions
â€¢ Past-date detector: regex r'(20\d{2})\s*áƒ¬áƒ”áƒš'
  â†’ sets temporal_warning = True, extracted_year = YYYY

DONE WHEN: 3 classifiers work independently, pass unit tests

Tests (6):
  1. "áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ áƒ¡áƒáƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒšáƒ?" â†’ disclaimer_needed = True (Red Zone)
  2. "áƒ áƒ áƒáƒ áƒ˜áƒ¡ áƒ“áƒ¦áƒ’?" â†’ disclaimer_needed = False (informational, no amount)
  3. "2022 áƒ¬áƒ”áƒšáƒ¡ áƒ’áƒáƒ•áƒ§áƒ˜áƒ“áƒ”" â†’ temporal_warning = True, extracted_year = 2022
  4. "áƒ˜áƒœáƒ“áƒ˜áƒ•áƒ˜áƒ“áƒ£áƒáƒšáƒ£áƒ áƒ˜ áƒ›áƒ”áƒ¬áƒáƒ áƒ›áƒ”" â†’ term resolver enriches with definition from DB
  5. Query without any known term â†’ term resolver returns empty (no crash)
  6. "áƒ›áƒáƒ›áƒáƒ•áƒáƒš áƒ¬áƒ”áƒšáƒ¡" (no year digits) â†’ temporal_warning = False
```

#### Task 6b: Georgian System Prompt
```
â€¢ tax_system_prompt.py with rules:
  - "áƒ›áƒ£áƒ®áƒšáƒ˜áƒ¡ áƒªáƒ˜áƒ¢áƒ˜áƒ áƒ”áƒ‘áƒ áƒ¡áƒáƒ•áƒáƒšáƒ“áƒ”áƒ‘áƒ£áƒšáƒáƒ"
  - "áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ áƒ˜ áƒœáƒáƒ áƒ›áƒ áƒ¯áƒáƒ‘áƒœáƒ˜áƒ¡ áƒ–áƒáƒ’áƒáƒ“áƒ¡ â€” áƒ§áƒáƒ•áƒ”áƒšáƒ—áƒ•áƒ˜áƒ¡ áƒáƒ®áƒ¡áƒ”áƒœáƒ” áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ”áƒ‘áƒ˜"
  - "áƒ›áƒ áƒáƒ•áƒáƒšáƒ˜ áƒáƒ˜áƒ áƒáƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒ¨áƒ”áƒáƒ›áƒáƒ¬áƒ›áƒ” áƒ§áƒ•áƒ”áƒšáƒ áƒ›áƒ£áƒ®áƒšáƒ˜"
  - "áƒáƒ áƒáƒ¡áƒáƒ“áƒ”áƒ¡ áƒ’áƒáƒ£áƒ¬áƒ˜áƒ áƒ áƒ©áƒ”áƒ•áƒ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒ áƒ˜áƒ“áƒ”áƒ‘áƒáƒ–áƒ”"
  - "áƒ—áƒ£ áƒ¥áƒ•áƒ”áƒ™áƒáƒœáƒáƒœáƒ£áƒ áƒ˜ áƒáƒ¥áƒ¢áƒ˜ áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ áƒ˜áƒ, áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒ” áƒœáƒáƒ›áƒ”áƒ áƒ˜"
â€¢ Prompt assembly: system + disclaimer + definitions + temporal warning + context + question

DONE WHEN: Prompt template renders with all conditional sections

Tests (4):
  1. Prompt with disclaimer=True â†’ output contains "âš ï¸" warning text
  2. Prompt with disclaimer=False â†’ no warning block in output
  3. Prompt with definitions=["term1"] â†’ output contains definition section
  4. Prompt with empty context (no articles found) â†’ renders gracefully (no crash)
```

#### Task 6c: answerQuestion() Core Pipeline
```
â€¢ Pipeline: classify â†’ resolve terms â†’ embed â†’ search â†’ enrich refs â†’ rerank â†’ prompt â†’ LLM
â€¢ Uses Gemini generate_content with system_instruction
â€¢ Returns: {answer: str, sources: List[int], disclaimer: bool, temporal_warning: bool}

DONE WHEN: End-to-end query returns structured response with cited articles

Tests (5):
  1. "áƒ“áƒ¦áƒ’ áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ?" â†’ response.sources includes article 169
  2. Response always has {answer, sources, disclaimer, temporal_warning} keys
  3. Empty search results (áƒ¡áƒ™áƒáƒ  < 0.65) â†’ answer contains "áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ”" (not empty string)
  4. LLM API timeout (mocked) â†’ returns {error: "LLM_ERROR"} (not 500)
  5. Malformed LLM response (no article citation) â†’ source list empty, answer still returned
```

#### Task 6d: Conversation History + Sub-Legislative Refs
```
â€¢ Inject last N turns for context chain (multi-turn awareness)
â€¢ Sub-legislative refs: inject from hardcoded top-10 when relevant
â€¢ Partial failure handling: if LLM call fails, return error response (not 500)

DONE WHEN: Q1="áƒ“áƒ¦áƒ’?" Q2="áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜?" â†’ second answer knows context is VAT

Tests (4):
  1. Single turn: Q="áƒ“áƒ¦áƒ’?" â†’ answer about VAT rate
  2. Multi-turn: Q1="áƒ“áƒ¦áƒ’?" Q2="áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜?" â†’ Q2 answer is about VAT exceptions (not income tax)
  3. LLM call fails (mocked) â†’ returns {error: "LLM_ERROR", code: "LLM_ERROR"} (not raw 500)
  4. History with 20+ turns â†’ only last N injected (no context overflow)
```

#### Task 6e: Integration Tests (8 scenarios)
```
â€¢ Tests:
  1. "áƒ“áƒ¦áƒ’-áƒ¡ áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜?" â†’ "18%" + áƒ›áƒ£áƒ®áƒšáƒ˜ 169
  2. "áƒ áƒ áƒáƒ áƒ˜áƒ¡ áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜?" â†’ disambiguates by chapter (hierarchy)
  3. "áƒ›áƒáƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜?" â†’ cites 98 + 100 (cross-ref)
  4. "áƒ áƒáƒ’áƒáƒ  áƒ“áƒáƒ•áƒ›áƒáƒšáƒ?" â†’ refusal (guardrail)
  5. Q1="áƒ“áƒ¦áƒ’?" Q2="áƒ’áƒáƒ›áƒáƒœáƒáƒ™áƒšáƒ˜áƒ¡áƒ˜?" â†’ context chain
  6. "áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ áƒ¡áƒáƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒšáƒ?" â†’ 20% + 1% exception (lex specialis)
  7. "áƒ¤áƒ˜áƒ–áƒ˜áƒ™áƒ£áƒ  áƒáƒ˜áƒ áƒ¡ áƒ¨áƒ”áƒ£áƒ«áƒšáƒ˜áƒ áƒ“áƒ¦áƒ’?" â†’ resolves "áƒ“áƒáƒ¡áƒáƒ‘áƒ”áƒ’áƒ áƒ˜ áƒáƒ˜áƒ áƒ˜" (terminology)
  8. "2022 áƒ¬áƒ”áƒšáƒ¡ áƒ’áƒáƒ•áƒ§áƒ˜áƒ“áƒ”" â†’ disclaimer (temporal)
â€¢ ğŸ”´ Embedding Quality Validation (post-seed):
  VALIDATION_QUERIES = [
    ("áƒ‘áƒ˜áƒœáƒ áƒ áƒáƒ› áƒ’áƒáƒ•áƒ§áƒ˜áƒ“áƒ áƒ áƒ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ˜áƒ?", [168]),
    ("áƒ“áƒ¦áƒ’-áƒ¡ áƒ’áƒáƒœáƒáƒ™áƒ•áƒ”áƒ—áƒ˜ áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ?", [169]),
    ("áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ áƒ¡áƒáƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒšáƒ?", [180]),
    ("áƒáƒ¥áƒªáƒ˜áƒ–áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ˜ áƒšáƒ£áƒ“áƒ–áƒ”", [188]),
    ("áƒáƒ˜áƒ áƒ’áƒáƒ¡áƒáƒ›áƒ¢áƒ”áƒ®áƒšáƒ áƒ“áƒáƒ’áƒ•áƒ˜áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡", [272]),
  ]
  â†’ All 5 must return correct article in top-3

DONE WHEN: All 8 tests pass + 5/5 validation queries return correct article in top-3
```

### Task 7: API Routes + SSE Streaming + Sessions + Auth

> [!IMPORTANT]
> All 3 features (SSE, sessions, auth) **already exist in Scoop backend** â€” we COPY & ADAPT.
> Total adaptation: ~3.5 hours. Frontend reuses `useSSEStream`, `apiClient`, `useSessionStore` as-is.

#### 7a: Core Endpoints (G3, G8, G9)

**POST /api/ask** â€” Synchronous query (for simple clients)
```json
// Request
{"question": "string (1-500 chars)", "conversation_id": "string? (optional)"}

// Success Response (200)
{
  "answer": "string",
  "sources": [{"article_number": 169, "title": "...", "score": 0.84}],
  "disclaimer": true,
  "temporal_warning": false
}

// Error Response (4xx/5xx)
{"error": "string", "code": "INVALID_INPUT|RATE_LIMITED|LLM_ERROR|DB_ERROR"}
```

**GET /api/articles/{number}** â€” Direct article lookup
```json
// Response (200)
{"article_number": 169, "kari": "...", "tavi": "...", "title": "...", "body": "..."}

// Response (404)
{"error": "Article not found", "code": "NOT_FOUND"}
```

**GET /api/health** â€” Health check
```json
{"status": "ok", "db_connected": true, "articles_count": 312, "last_sync": "2026-02-16T..."}
```

#### 7b: SSE Streaming (COPY from Scoop `main.py:1553`)

**POST /api/ask/stream** â€” Streaming query (for frontend)
```
// Same request body as POST /api/ask
// Response: SSE stream with adapted event types:

event: thinking\ndata: {"step": "Searching tax articles..."}\n\n
event: sources\ndata: [{"article_number": 169, "title": "...", "score": 0.84}]\n\n
event: text\ndata: {"content": "partial answer chunk..."}\n\n
event: disclaimer\ndata: {"show": true, "temporal_warning": false}\n\n
event: done\ndata: {"conversation_id": "uuid"}\n\n
event: error\ndata: {"error": "...", "code": "LLM_ERROR"}\n\n
```

**Adaptation from Scoop:** Copy `StreamingResponse` + event generator. Change event types:
- `products` â†’ `sources` (tax article citations)
- `tip` â†’ `disclaimer` (legal disclaimer + temporal warning)
- Keep: `text`, `thinking`, `done`, `error`
- Drop: `quick_replies`, `truncation_warning`

#### 7c: Session Endpoints (COPY from Scoop `main.py:1559-1577`)

> **MongoDB collection:** `conversations` in `georgian_tax_db` (NOT scoop_db)
> Adapted from Scoop's `conversation_store.py`. Schema:
> `{conversation_id: str, user_id: str, title: str, turns: [{role, content, timestamp}], created_at: ISODate, updated_at: ISODate}`

**GET /api/sessions** â€” List user conversations
```json
[{"conversation_id": "uuid", "title": "áƒ“áƒ¦áƒ’-áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒ®áƒ”áƒ‘", "created_at": "...", "updated_at": "..."}]
```

**GET /api/session/{id}/history** â€” Load conversation history
```json
{"conversation_id": "uuid", "turns": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

**POST /api/session/clear** â€” Clear conversation data
```json
// Response (200)
{"cleared": true}
```

**Source:** Wrappers around Task 6d's extracted conversation CRUD (from `conversation_store.py`).

#### 7d: Auth Middleware (COPY from Scoop `app/auth/`)

> **MongoDB collection:** `api_keys` in `georgian_tax_db` (NOT scoop_db)
> Schema: `{key_hash: str, created_at: ISODate, last_used: ISODate, is_active: bool}`

- API key enrollment: `POST /api/auth/enroll` â†’ generates + returns API key
- Validation: `X-API-Key` header check on all protected endpoints (/ask, /sessions)
- `/health` and `/api/auth/enroll` remain public (no key required)
- Rate limiting: 30 req/min via slowapi (on top of auth)

```
â€¢ Implementation:
  - POST /api/ask â†’ tax_agent.answerQuestion() â€” validate input len 1-500
  - POST /api/ask/stream â†’ same pipeline, wrapped in StreamingResponse (COPY from Scoop)
  - GET /api/articles/{number} â†’ find_by_number() â€” validate 1-500 range
  - GET /api/health â†’ DB status + articles count + last sync date
  - GET /api/sessions â†’ list conversations for user (from Task 6d CRUD)
  - GET /api/session/{id}/history â†’ load conversation turns (from Task 6d CRUD)
  - POST /api/session/clear â†’ clear conversation (from Task 6d CRUD)
  - POST /api/auth/enroll â†’ generate API key (COPY from Scoop app/auth/)
  - Auth: X-API-Key middleware on /ask, /sessions (COPY from Scoop â€” ~30 min)
  - Rate limiting: 30 req/min via slowapi
  - CORS: CORSMiddleware already in main.py (from Task 1)
  - Error handling: all exceptions â†’ structured error response (never raw 500)

DONE WHEN: All 8 endpoints return correct JSON, SSE stream works end-to-end, auth blocks unauthorized, rate limiter triggers at 31st request

Tests (15):
  Core:
  1. POST /api/ask with valid question â†’ 200 + structured response
  2. POST /api/ask with empty question â†’ 422 + {error, code: "INVALID_INPUT"}
  3. POST /api/ask with question >500 chars â†’ 422 + {error, code: "INVALID_INPUT"}
  4. GET /api/articles/169 â†’ 200 + article JSON with body
  5. GET /api/articles/9999 â†’ 404 + {error: "Article not found", code: "NOT_FOUND"}
  6. GET /api/health â†’ 200 + {status, db_connected, articles_count, last_sync}
  7. 31st request within 1 minute â†’ 429 + {error, code: "RATE_LIMITED"}
  SSE:
  8. POST /api/ask/stream â†’ SSE stream with event: text chunks
  9. POST /api/ask/stream â†’ stream includes event: sources before text
  10. POST /api/ask/stream â†’ stream ends with event: done
  11. POST /api/ask/stream with bad input â†’ event: error emitted
  Sessions:
  12. GET /api/sessions â†’ 200 + list of conversations
  13. GET /api/session/{id}/history â†’ 200 + conversation turns
  14. POST /api/session/clear â†’ 200 + {cleared: true}
  Auth:
  15. Request without X-API-Key to /api/ask â†’ 401 Unauthorized
```

### Task 8: Seed + Sync Scripts
```
â€¢ seed_database.py: scrape all â†’ embed all â†’ bulk insert
  - Uses detect_latest_version() â€” NOT hardcoded publication number
  - Stores detected version in metadata collection after successful seed
  - Idempotency: uses upsert (keyed on article_number) â€” safe to re-run
  - Atomic operation: if embedding fails mid-batch, already-inserted articles remain
    (no need to re-scrape, just re-embed failed batch)
â€¢ sync_matsne.py: periodic version check â†’ re-scrape if new version found
  - check_for_new_version(): GET base URL â†’ follow redirect â†’ compare publication N
  - If N > stored_version â†’ trigger full re-scrape + re-embed (upsert = safe)
  - Log: "ğŸ†• New Tax Code version detected: {N} (stored: {stored_N})"
  - Run: daily cron OR on server startup
â€¢ Canary test after seed: query "áƒ›áƒ£áƒ®áƒšáƒ˜ 160" returns result
â€¢ Run embedding quality validation (from Task 6e) after seed

DONE WHEN: seed_database.py runs end-to-end, canary passes, re-run is idempotent

Tests (5):
  1. seed_database.py full run â†’ inserts 300+ articles in tax_articles collection
  2. Re-run seed_database.py â†’ same article count (upsert, no duplicates)
  3. Canary: after seed, query "áƒ›áƒ£áƒ®áƒšáƒ˜ 160" â†’ returns 1 result
  4. metadata collection has {key: "tax_code_version", publication: N>200, scrape_status: "completed", embedding_model: "text-embedding-004"}
  5. sync_matsne.py with same version â†’ logs "no update needed" (no re-scrape)
```

---

## Execution Order

> [!IMPORTANT]
> **Sequential:** Task 3.0 (spike) â†’ 1 â†’ 2 â†’ 3a â†’ 3b â†’ 3c â†’ 3d â†’ 3e â†’ 3f â†’ 4 â†’ 5 â†’ 6a â†’ 6b â†’ 6c â†’ 6d â†’ 8 (seed) â†’ 6e (needs seeded data) â†’ 7
> The validation spike determines scraper approach. The scraper (Task 3) determines data format for everything downstream.

---

## Rollback Strategy (G4b)

| Failure Point | Rollback Action | Data Loss? |
|---|---|---|
| Task 3 spike fails (JS-rendered) | Add playwright to requirements, re-plan 3a | None |
| Scraping fails mid-run | Already-inserted articles remain (upsert). Fix + re-run | None |
| Embedding batch fails | Re-embed only failed batch (articles without embeddings) | None |
| Atlas index creation fails | Retry via Atlas UI. No code change needed | None |
| Seed produces bad data | Drop `tax_articles` collection + re-seed (upsert is idempotent) | Rebuild ~15min |
| LLM returns garbage | Adjust system prompt in `tax_system_prompt.py` (isolated file) | None |
| Threshold too aggressive | Change `SIMILARITY_THRESHOLD` env var, no redeploy | None |

---

## Phase 2 Roadmap (Post-MVP)

| Feature | Description | Effort |
|---|---|---|
| **Full Temporal Search** | 239 historical versions via `publication=N` â€” see details below | **XL** |
| Sub-Legislative Scraping | áƒ¤áƒ˜áƒœáƒáƒœáƒ¡áƒ—áƒ áƒ›áƒ˜áƒœáƒ˜áƒ¡áƒ¢áƒ áƒ˜áƒ¡ áƒ‘áƒ áƒ«áƒáƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜ â†’ `sub_legislative` collection | L |
| User Profiles | "áƒ˜áƒœáƒ“. áƒ›áƒ”áƒ¬áƒáƒ áƒ›áƒ” áƒ®áƒáƒ  áƒ—áƒ£ áƒ¨áƒáƒ¡?" â†’ personalized answers | M |
| Streaming (SSE) | âœ… **Moved to Phase 1** â€” copied from Scoop backend | â€” |
| Calculator Integration | Structured calc for simple tax scenarios | L |

### Phase 2 Detail: Full Temporal Search (239 Versions)

> [!IMPORTANT]
> **Validated:** Matsne exposes all 239 historical versions via URL parameter `?publication=N`.
> Same HTML structure (`a.oldStyleDocumentPart`) â€” our BS4 parser works on all versions unchanged.

**URL pattern:**
```
https://matsne.gov.ge/ka/document/view/1043717?publication={1..239}
```

| publication | Date Range | Content |
|---|---|---|
| `1` | 07/12/2010 | áƒáƒ áƒ˜áƒ’áƒ˜áƒœáƒáƒšáƒ˜ |
| `~200` | ~2022 | áƒ¨áƒ£áƒáƒšáƒ”áƒ“áƒ£áƒ áƒ˜ |
| `239` | áƒ›áƒáƒ¥áƒ›áƒ”áƒ“áƒ˜ | áƒ‘áƒáƒšáƒ áƒ™áƒáƒœáƒ¡áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ |

**Implementation:**
```python
# 1. Scrape all 239 versions (one-time seed, ~4hrs with rate limiting)
for pub in range(1, 240):
    articles = scrape(f"...?publication={pub}")
    for art in articles:
        art.version = pub
        art.valid_from = extract_date(pub)      # from page header
        art.valid_to = extract_date(pub + 1)    # next version's date

# 2. New collection: tax_articles_history (separate from active)
# 3. Query: "2022 áƒ¬áƒ”áƒšáƒ¡ áƒ’áƒáƒ•áƒ§áƒ˜áƒ“áƒ” áƒ‘áƒ˜áƒœáƒ" â†’ find version where valid_from â‰¤ 2022 â‰¤ valid_to
# 4. Compare: show diff between then and now
```

**Storage estimate:** ~239 Ã— 300 articles Ã— ~2KB = **~140MB** in MongoDB â€” manageable.
